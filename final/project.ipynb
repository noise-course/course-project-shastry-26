{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29934a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Samples: 25798it [00:15, 1687.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import dpkt\n",
    "import pcapml_fe\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset to load\n",
    "PCAPML_PATH = \"traffic.pcapng\"\n",
    "\n",
    "# Directory to save individual pcaps\n",
    "PCAP_DIR = \"pcaps\"\n",
    "\n",
    "# Directory to save labels\n",
    "LABELS_CSV = \"labels.csv\"\n",
    "\n",
    "os.makedirs(PCAP_DIR, exist_ok=True)\n",
    "\n",
    "# convert sample metadata into a string\n",
    "def extract_label(sample):\n",
    "    meta = sample.metadata\n",
    "    parts = []\n",
    "    for p in meta.split(\",\"):\n",
    "        cleaned = p.strip()\n",
    "        if cleaned:\n",
    "            parts.append(cleaned)\n",
    "\n",
    "    lf = parts[0]\n",
    "\n",
    "    if \"-\" in lf:\n",
    "        lf = lf.split(\"-\", 1)[0]\n",
    "\n",
    "    return lf.strip()\n",
    "\n",
    "with open(LABELS_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Item\", \"Label\"])\n",
    "\n",
    "    for sample in tqdm(pcapml_fe.sampler(PCAPML_PATH), desc=\"Samples\"):\n",
    "        sid = str(sample.sid)\n",
    "        label = extract_label(sample)\n",
    "        pcap_name = f\"{sid}.pcap\"\n",
    "        pcap_path = os.path.join(PCAP_DIR, pcap_name)\n",
    "\n",
    "        pkts = sample.packets\n",
    "\n",
    "        # Find earliest timestamp in this sample to normalize\n",
    "        ts0 = float(\"inf\")\n",
    "        for p in pkts:\n",
    "            ts0 = min(ts0, p.ts)\n",
    "\n",
    "        with open(pcap_path, \"wb\") as pcap_f:\n",
    "            # write pcap file per sample\n",
    "            w = dpkt.pcap.Writer(pcap_f)\n",
    "            for pkt in pkts:\n",
    "                # dpkt requires positive timestamp so we normalize with ts0\n",
    "                norm_ts = max(0,pkt.ts - ts0)\n",
    "                w.writepkt(pkt.raw_bytes, ts=norm_ts)\n",
    "\n",
    "        writer.writerow([pcap_name, label])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set 7 arbitrarily but the idea is to create a standard train matrix size\n",
    "# Example command\n",
    "# nprint -4 -t -c 7 -P pcaps/273992.pcap | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11789bb",
   "metadata": {},
   "source": [
    "I am not sure if different nprint versions have different command structures, however, here is the help menu from the version I am running (1.2.1):\n",
    "```\n",
    "-4, --ipv4                 include ipv4 headers\n",
    "-6, --ipv6                 include ipv6 headers\n",
    "-A, --absolute_timestamps  include absolute timestmap field\n",
    "-c, --count=INTEGER        number of packets to parse (if not all)\n",
    "-C, --csv_file=FILE        csv (hex packets) infile\n",
    "-d, --device=STRING        device to capture from if live capture\n",
    "-e, --eth                  include eth headers\n",
    "-f, --filter=STRING        filter for libpcap\n",
    "-F, --fill_int=INT8_T      integer to fill missing bits with\n",
    "-h, --nprint_filter_help   print regex possibilities\n",
    "-i, --icmp                 include icmp headers\n",
    "-N, --nPrint_file=FILE     nPrint infile\n",
    "-O, --write_index=INTEGER  Output file Index (first column) Options:\n",
    "                            \n",
    "                            0: source IP (default)\n",
    "                            \n",
    "                            1: destination IP\n",
    "                            \n",
    "                            2: source port\n",
    "                            \n",
    "                            3: destination port\n",
    "                            \n",
    "                            4: flow (5-tuple)\n",
    "                            \n",
    "                            5: wlan tx mac\n",
    "-p, --payload=PAYLOAD_SIZE include n bytes of payload\n",
    "-P, --pcap_file=FILE       pcap infile\n",
    "-r, --radiotap             include radiotap headers\n",
    "-R, --relative_timestamps  include relative timestamp field\n",
    "-S, --stats                print stats about packets processed when finished\n",
    "-t, --tcp                  include tcp headers\n",
    "-u, --udp                  include udp headers\n",
    "-V, --verbose              print human readable packets with nPrints\n",
    "-w, --wlan                 include wlan headers\n",
    "-W, --write_file=FILE      file for output, else stdout\n",
    "-x, --nprint_filter=STRING regex to filter bits out of nPrint. nprint -h for\n",
    "                            details\n",
    "-?, --help                 Give this help list\n",
    "    --usage                Give a short usage message\n",
    "    --version              Print program version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803baaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25798/25798 [10:11<00:00, 42.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25798, 6720)\n",
      "y shape: (25798,)\n",
      "labels: {'avtech': 0, 'huawei': 1, 'roku': 2, 'axis': 3, 'h3c': 4, 'lancom': 5, 'mikrotik': 6, 'dell': 7, 'juniper': 8, 'cisco': 9, 'zte': 10, 'nec': 11, 'adtran': 12, 'ubiquoss': 13, 'chromecast': 14}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import io\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "PCAP_DIR = \"pcaps\"\n",
    "LABELS_CSV = \"labels.csv\"\n",
    "OUT_X = \"X_nprint.npy\"\n",
    "OUT_Y = \"y_nprint.npy\"\n",
    "OUT_LABEL_MAP = \"label_map_nprint.txt\"\n",
    "\n",
    "N_PACKETS = 7\n",
    "\n",
    "def nprint_vector(pcap_path, n_packets=N_PACKETS):\n",
    "    # same command as above\n",
    "    cmd = [\"nprint\", \"-4\", \"-t\", \"-c\", str(n_packets), \"-P\", pcap_path]\n",
    "    try:\n",
    "        result = subprocess.run(cmd,check=True,capture_output=True,text=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    text = result.stdout.strip()\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    #nprint outputs csv\n",
    "    reader = csv.reader(io.StringIO(text))\n",
    "    rows = list(reader)\n",
    "    if len(rows) <= 1:\n",
    "        return None\n",
    "\n",
    "    header = rows[0]\n",
    "    # discarding the index col\n",
    "    n_features = len(header) - 1\n",
    "    data_rows = rows[1:]\n",
    "\n",
    "    packet_feats = []\n",
    "    # Want to create uniform sized trainint/testing sets so truncate at n_packets\n",
    "    for row in data_rows[:n_packets]:\n",
    "        # drop index\n",
    "        feat_vals = row[1:]\n",
    "\n",
    "        # Add padding if necessary (gpt made this if statement)\n",
    "        if len(feat_vals) < n_features:\n",
    "            feat_vals += [\"-1\"]*(n_features-len(feat_vals))\n",
    "        feats = []\n",
    "\n",
    "        for v in feat_vals:\n",
    "            feats.append(int(v))\n",
    "        packet_feats.append(feats)\n",
    "\n",
    "    # suppose we have less than n_packets in this sample, we just pad by adding -1s\n",
    "    while len(packet_feats) < n_packets:\n",
    "        packet_feats.append([-1]* n_features)\n",
    "\n",
    "    # Return the features as a flattened array\n",
    "    return np.array(packet_feats, dtype=np.int8).flatten()\n",
    "\n",
    "def _worker(job):\n",
    "    pcap_path, label_str = job\n",
    "    vec = nprint_vector(pcap_path)\n",
    "    return vec, label_str\n",
    "\n",
    "def build_nprint_dataset_parallel():\n",
    "    with open(LABELS_CSV) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "\n",
    "    jobs = []\n",
    "    for r in rows:\n",
    "        pcap_path = os.path.join(PCAP_DIR, r[\"Item\"])\n",
    "        label_str = r[\"Label\"]\n",
    "        jobs.append((pcap_path, label_str))\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    labels_seen = {}\n",
    "\n",
    "    # Used GPT to modify this function to be multithreaded since it was taking so long to run\n",
    "    with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "        for vec, label_str in tqdm(ex.map(_worker, jobs), total=len(jobs)):\n",
    "            if vec is None:\n",
    "                continue\n",
    "            \n",
    "            # idea is to create a new label mapping every time we see a new label\n",
    "            if label_str not in labels_seen:\n",
    "                labels_seen[label_str] = len(labels_seen)\n",
    "\n",
    "            X_list.append(vec)\n",
    "            y_list.append(labels_seen[label_str])\n",
    "\n",
    "    # X is now a matrix with each row representing a new sample and the row containing the flattened 7 nprint representations\n",
    "    X = np.stack(X_list)\n",
    "    # These are the corresponding labels (as integer class ids)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    return X, y, labels_seen\n",
    "\n",
    "# build and save data\n",
    "# I decided to save the dataset since this process takes so long\n",
    "X, y, label_map = build_nprint_dataset_parallel()\n",
    "np.save(OUT_X, X)\n",
    "np.save(OUT_Y, y)\n",
    "\n",
    "# write the label map too\n",
    "with open(OUT_LABEL_MAP, \"w\") as f:\n",
    "    for label, idx in label_map.items():\n",
    "        f.write(f\"{idx},{label}\\n\")\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"labels:\", label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52280d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load saved data for next step\n",
    "X = np.load(\"X_nprint.npy\")\n",
    "y = np.load(\"y_nprint.npy\")\n",
    "\n",
    "# create the train test split (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2230702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8940645321678519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       430\n",
      "           1       0.80      0.75      0.78       282\n",
      "           2       0.85      0.94      0.89       481\n",
      "           3       0.85      0.95      0.90       531\n",
      "           4       0.88      0.81      0.84       276\n",
      "           5       0.94      0.92      0.93       285\n",
      "           6       0.81      0.67      0.73       272\n",
      "           7       0.85      0.87      0.86       290\n",
      "           8       0.96      0.92      0.94       289\n",
      "           9       0.97      0.94      0.95       290\n",
      "          10       0.96      0.94      0.95       285\n",
      "          11       0.98      0.99      0.98       290\n",
      "          12       0.98      0.98      0.98       290\n",
      "          13       0.98      0.93      0.95       295\n",
      "          14       0.99      0.98      0.99       574\n",
      "\n",
      "    accuracy                           0.90      5160\n",
      "   macro avg       0.91      0.89      0.90      5160\n",
      "weighted avg       0.90      0.90      0.90      5160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First trying with a random forest classifer\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0787df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.78500820\n",
      "Validation score: 0.610950\n",
      "Iteration 2, loss = 1.13859623\n",
      "Validation score: 0.730136\n",
      "Iteration 3, loss = 0.90572200\n",
      "Validation score: 0.773740\n",
      "Iteration 4, loss = 0.74436790\n",
      "Validation score: 0.804748\n",
      "Iteration 5, loss = 0.65368186\n",
      "Validation score: 0.805233\n",
      "Iteration 6, loss = 0.59900762\n",
      "Validation score: 0.823159\n",
      "Iteration 7, loss = 0.54989130\n",
      "Validation score: 0.843992\n",
      "Iteration 8, loss = 0.51773694\n",
      "Validation score: 0.829942\n",
      "Iteration 9, loss = 0.48238573\n",
      "Validation score: 0.833333\n",
      "Iteration 10, loss = 0.46053538\n",
      "Validation score: 0.866764\n",
      "Iteration 11, loss = 0.44340787\n",
      "Validation score: 0.815891\n",
      "Iteration 12, loss = 0.42075833\n",
      "Validation score: 0.814438\n",
      "Iteration 13, loss = 0.39405591\n",
      "Validation score: 0.860950\n",
      "Iteration 14, loss = 0.38429296\n",
      "Validation score: 0.875969\n",
      "Iteration 15, loss = 0.35442227\n",
      "Validation score: 0.868217\n",
      "Iteration 16, loss = 0.37086689\n",
      "Validation score: 0.811047\n",
      "Iteration 17, loss = 0.37177727\n",
      "Validation score: 0.846899\n",
      "Iteration 18, loss = 0.33650440\n",
      "Validation score: 0.873547\n",
      "Iteration 19, loss = 0.32500761\n",
      "Validation score: 0.886628\n",
      "Iteration 20, loss = 0.32566946\n",
      "Validation score: 0.862888\n",
      "Iteration 21, loss = 0.29952819\n",
      "Validation score: 0.871124\n",
      "Iteration 22, loss = 0.29084857\n",
      "Validation score: 0.858527\n",
      "Iteration 23, loss = 0.30389713\n",
      "Validation score: 0.842054\n",
      "Iteration 24, loss = 0.28249602\n",
      "Validation score: 0.865310\n",
      "Iteration 25, loss = 0.27837533\n",
      "Validation score: 0.869671\n",
      "Iteration 26, loss = 0.28342533\n",
      "Validation score: 0.871124\n",
      "Iteration 27, loss = 0.26649018\n",
      "Validation score: 0.863372\n",
      "Iteration 28, loss = 0.29403163\n",
      "Validation score: 0.878391\n",
      "Iteration 29, loss = 0.26079009\n",
      "Validation score: 0.866279\n",
      "Iteration 30, loss = 0.23581904\n",
      "Validation score: 0.880814\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Balanced Accuracy: 0.8703195954507035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       430\n",
      "           1       0.84      0.63      0.72       282\n",
      "           2       0.83      0.94      0.88       481\n",
      "           3       0.84      0.91      0.88       531\n",
      "           4       0.80      0.80      0.80       276\n",
      "           5       0.90      0.93      0.92       285\n",
      "           6       0.72      0.68      0.70       272\n",
      "           7       0.80      0.88      0.84       290\n",
      "           8       0.91      0.90      0.90       289\n",
      "           9       0.97      0.92      0.94       290\n",
      "          10       0.89      0.94      0.91       285\n",
      "          11       0.98      0.99      0.98       290\n",
      "          12       1.00      0.95      0.97       290\n",
      "          13       1.00      0.81      0.89       295\n",
      "          14       0.99      0.98      0.99       574\n",
      "\n",
      "    accuracy                           0.88      5160\n",
      "   macro avg       0.88      0.87      0.87      5160\n",
      "weighted avg       0.88      0.88      0.88      5160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    # Since we have a pretty sparse matrix, scale by std not mean\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"mlp\", MLPClassifier(\n",
    "        hidden_layer_sizes=(256,128),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=30,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        verbose=True,\n",
    "        batch_size=256,\n",
    "    )),\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
